Sequential(
  (0): VTProcessInput(
    (conv_proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (1): AddPosEmbedding()
  (2): Dropout(p=0.0, inplace=False)
  (3): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (4): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (5): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (6): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (7): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (8): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (9): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (10): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (11): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (12): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (13): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (14): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (15): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (16): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (17): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (18): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (19): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (20): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (21): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (22): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (23): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (24): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (25): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (26): EncoderBlock(
    (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (add_layer): AddLayer()
    (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=4096, out_features=1024, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (27): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (28): ClassifierTokenLayer()
  (29): Linear(in_features=1024, out_features=1000, bias=True)
)